{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import typing as tp\n",
    "\n",
    "import wandb\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LLM_NAME: str = 'gpt2'\n",
    "VIT_NAME: str = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "VIT_EMB_SIZE = 1024\n",
    "LLM_INP_EMB_SIZE = 768\n",
    "LLM_OUT_EMB_SIZE = 768\n",
    "RET_EMB_SIZE = 600\n",
    "\n",
    "DATA_PATH: str = \"./\"\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "RET_TOKEN = \"[RET]\"\n",
    "PAD_TOKEN = \"<|endoftext|>\"\n",
    "IMG_CLS_TOKEN = \"<|image|>\"\n",
    "EOT_TOKEN = \"<|endoftext|>\"\n",
    "\n",
    "SESSION_SIZE = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for image captioning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRCaptioningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        data_type: str = \"train\",\n",
    "        transform: tp.Optional[albu.Compose | None] = None\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.data_type = data_type\n",
    "        self.data = pd.read_csv(DATA_PATH+data_type+\"_annotation_dataframe.csv\")\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def get_batch(self, batch_size: int) -> tp.Tuple:\n",
    "        rand_idxes = np.random.randint(0, len(self.data), batch_size)\n",
    "        \n",
    "        img_paths = self.data.loc[rand_idxes, \"Path\"].to_list()\n",
    "        annotations = self.data.loc[rand_idxes, \"Annotation\"].to_list()\n",
    "\n",
    "        imgs = torch.cat([\n",
    "            self._get_image(img_path)[None, ...] for img_path in img_paths\n",
    "        ])\n",
    "        annotations_tokens = self.tokenizer(annotations, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        return (imgs, annotations_tokens)\n",
    "    \n",
    "    def _get_image(self, image_path: str) -> torch.Tensor:\n",
    "        img = np.array(PIL.Image.open(image_path).convert(\"RGB\"))\n",
    "        if self.transform is None:\n",
    "            raise ValueError(\"Transformation must be at least ToTensor, but None recieved\")\n",
    "        img = self.transform(image=img)[\"image\"].float()\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index) -> tp.Tuple:\n",
    "        img_path, annotation = self.data.iloc[index]\n",
    "        \n",
    "        img = self._get_image(img_path)\n",
    "        annotation_tokens = self.tokenizer(annotation, return_tensors=\"pt\")\n",
    "    \n",
    "        return (\n",
    "            img,\n",
    "            annotation_tokens\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for visual question answering task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRVQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        data_type: str = \"train\",\n",
    "        transform: tp.Optional[albu.Compose | None] = None\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.data_type = data_type\n",
    "        self.data = pd.read_csv(DATA_PATH+data_type+\"_dataframe.csv\")\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.max_question_length = max([\n",
    "            len(ques) for ques in self.data[\"Question\"]\n",
    "        ])\n",
    "        self.max_answer_length = max([\n",
    "            len(ans) for ans in self.data[\"Answer\"]\n",
    "        ])\n",
    "    \n",
    "    def get_batch(self, batch_size: int) -> tp.Tuple:\n",
    "        rand_idxes = np.random.randint(0, len(self.data), batch_size)\n",
    "        \n",
    "        img_paths = self.data.loc[rand_idxes, \"Path\"].to_list()\n",
    "        questions = self.data.loc[rand_idxes, \"Question\"].to_list()\n",
    "        answers = self.data.loc[rand_idxes, \"Answer\"].to_list()\n",
    "        promt = [q + EOT_TOKEN + a for q, a in zip(questions, answers)]\n",
    "    \n",
    "        \n",
    "        imgs = torch.cat([\n",
    "            self._get_image(img_path)[None, ...] for img_path in img_paths\n",
    "        ])\n",
    "        \n",
    "        questions_tokens = self.tokenizer(questions, return_tensors=\"pt\", padding=True)\n",
    "        answers_tokens = self.tokenizer(answers, return_tensors=\"pt\", padding=True)\n",
    "        promt_tokens = self.tokenizer(promt, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        return (imgs, promt_tokens, questions_tokens, answers_tokens)\n",
    "    \n",
    "    def _get_image(self, image_path: str) -> torch.Tensor:\n",
    "        img = np.array(PIL.Image.open(image_path).convert(\"RGB\"))\n",
    "        if self.transform is None:\n",
    "            raise ValueError(\"Transformation must be at least ToTensor, but None recieved\")\n",
    "        img = self.transform(image=img)[\"image\"].float()\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index) -> tp.Tuple:\n",
    "        img_path, question, answer = self.data.iloc[index]\n",
    "        \n",
    "        img = self._get_image(img_path)\n",
    "        question_tokens = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        answer_tokens = self.tokenizer(answer, return_tensors=\"pt\")\n",
    "    \n",
    "        return (\n",
    "            img,\n",
    "            question_tokens,\n",
    "            answer_tokens\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model achitecture proposed in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_visual_tokens: int = 5\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(LLM_NAME)\n",
    "        self.llm = GPT2LMHeadModel.from_pretrained(LLM_NAME).to(DEVICE)\n",
    "        self.vit = CLIPVisionModel.from_pretrained(VIT_NAME).to(DEVICE)\n",
    "        self._add_special_tokens()\n",
    "        self._freeze_llm_vit()\n",
    "        \n",
    "        self.ret_token_idx = self.tokenizer.encode(RET_TOKEN)[0]\n",
    "        self.pad_token_idx = self.tokenizer.encode(PAD_TOKEN)[0]\n",
    "        \n",
    "        self.n_visual_tokens = n_visual_tokens\n",
    "        \n",
    "        self.vit2token = nn.Linear(VIT_EMB_SIZE, LLM_INP_EMB_SIZE * n_visual_tokens).to(DEVICE)\n",
    "        self.vit2retsapce = nn.Linear(VIT_EMB_SIZE, RET_EMB_SIZE).to(DEVICE)\n",
    "        self.llm2retspace = nn.Linear(LLM_OUT_EMB_SIZE, RET_EMB_SIZE).to(DEVICE)\n",
    "        self.input_embeddings = self.llm.get_input_embeddings()\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def decode_from_logits(self, logits):\n",
    "        return self.tokenizer.batch_decode(torch.argmax(logits, dim=2).to(\"cpu\"))\n",
    "    \n",
    "    def zero_grad_token_embeddings(self):\n",
    "        for param in self.llm.transformer.wte.parameters():\n",
    "            mask = torch.arange(param.grad.shape[0]) != self.ret_token_idx\n",
    "            param.grad[mask, :] = 0\n",
    "    \n",
    "    def zero_grad_llm_vit(self) -> None:\n",
    "        self.llm.zero_grad()\n",
    "        self.vit.zero_grad()\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        images,\n",
    "        question    \n",
    "    ):\n",
    "        vit_embeddings = self.vit(images)[\"pooler_output\"]\n",
    "        promt_embeddings = self.input_embeddings(question[\"input_ids\"])\n",
    "        \n",
    "        vit_tokens_embeddings = self.vit2token(\n",
    "            vit_embeddings\n",
    "        ).view(-1, self.n_visual_tokens, LLM_INP_EMB_SIZE)\n",
    "        \n",
    "        promt_embeddings = torch.cat([\n",
    "            vit_tokens_embeddings,\n",
    "            promt_embeddings\n",
    "        ], dim=1)\n",
    "        \n",
    "        llm_output = self.llm.generate(\n",
    "            inputs_embeds=promt_embeddings,\n",
    "            output_hidden_states=True,\n",
    "            pad_token_id=self.pad_token_idx,\n",
    "            max_length=len(promt_embeddings) + 10,\n",
    "        )\n",
    "        \n",
    "        return llm_output\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        text,\n",
    "        model_mode: str = \"captioning\"\n",
    "    ):\n",
    "\n",
    "        vit_embeddings = self.vit(images)[\"pooler_output\"]  \n",
    "        promt_embeddings = self.input_embeddings(text[\"input_ids\"])\n",
    "        \n",
    "        labels = text[\"input_ids\"]\n",
    "        attention_mask = text[\"attention_mask\"]\n",
    "        \n",
    "        if model_mode == \"captioning\":\n",
    "            vit_tokens_embeddings = self.vit2token(\n",
    "                vit_embeddings\n",
    "            ).view(-1, self.n_visual_tokens, LLM_INP_EMB_SIZE)\n",
    "            \n",
    "            promt_embeddings = torch.cat([\n",
    "                vit_tokens_embeddings,\n",
    "                promt_embeddings\n",
    "            ], dim=1)\n",
    "            \n",
    "            img_attention = torch.ones(\n",
    "                (attention_mask.size()[0], self.n_visual_tokens),\n",
    "                dtype=torch.long\n",
    "            ).to(DEVICE)\n",
    "            images_labels = (torch.zeros_like(img_attention) - 100).to(DEVICE)\n",
    "            \n",
    "            attention_mask = torch.cat([img_attention, attention_mask], dim=1)\n",
    "            labels = torch.cat([images_labels, labels], dim=1)\n",
    "\n",
    "        llm_output = self.llm(\n",
    "            inputs_embeds=promt_embeddings,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        return llm_output\n",
    "            \n",
    "    def _add_special_tokens(self) -> None: \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def _freeze_llm_vit(self):\n",
    "        self.llm.eval()\n",
    "        self.vit.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mega_model = MegaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = albu.Compose([\n",
    "    albu.Resize(\n",
    "        height=224, \n",
    "        width=224\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_dataset = CLEVRVQADataset(\n",
    "    tokenizer=my_mega_model.tokenizer,\n",
    "    data_type=\"train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_annotation_dataset = CLEVRCaptioningDataset(\n",
    "    tokenizer=my_mega_model.tokenizer,\n",
    "    data_type=\"train\",\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "opt = SGD(my_mega_model.parameters(), lr=lr)\n",
    "lambda1 = lambda epoch: 0.8 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"VQA\",\n",
    "    name=\"Image captioning task\",    \n",
    "    config={\n",
    "    \"architecture\": \"Frozen transformers\",\n",
    "    \"dataset\": \"CLEVR_v1.0\",\n",
    "    \"session size\": SESSION_SIZE,\n",
    "    \"batch\": BATCH_SIZE,\n",
    "    \"initial learning rate\": lr\n",
    "    }\n",
    ")\n",
    "\n",
    "predicted_annotations = []\n",
    "for iter_num in range(SESSION_SIZE):\n",
    "    imgs, annotation = train_annotation_dataset.get_batch(BATCH_SIZE)\n",
    "\n",
    "    annotation[\"attention_mask\"] = torch.zeros_like(annotation[\"attention_mask\"])\n",
    "    my_mega_model.zero_grad()\n",
    "    \n",
    "    output = my_mega_model(\n",
    "        images=imgs.to(DEVICE),\n",
    "        text=annotation.to(DEVICE),\n",
    "        model_mode=\"captioning\"\n",
    "    )\n",
    "    \n",
    "    output.loss.backward()\n",
    "    my_mega_model.zero_grad_llm_vit()\n",
    "    opt.step()\n",
    "    \n",
    "    if iter_num % 10 == 0:\n",
    "        predicted_annotations += [(\n",
    "            my_mega_model.decode_from_logits(output.logits.detach()),\n",
    "            my_mega_model.tokenizer.batch_decode(annotation[\"input_ids\"])\n",
    "        )]\n",
    "    \n",
    "    if (iter_num + 1) % (SESSION_SIZE / 10) == 0:\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_value = output.loss.detach().cpu().item()\n",
    "    writer.add_scalar(\"Loss\", loss_value, iter_num)\n",
    "    wandb.log({\"loss\": loss_value})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_annotations[-1][0][0])\n",
    "print(predicted_annotations[-1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, promt, question, answer = train_dataset.get_batch(BATCH_SIZE)\n",
    "\n",
    "generated_tokens = my_mega_model(\n",
    "    images=imgs.to(DEVICE),\n",
    "    text=question.to(DEVICE),\n",
    "    model_mode=\"captioning\"\n",
    ")\n",
    "\n",
    "generated_output = my_mega_model.generate(\n",
    "    images=imgs.to(DEVICE),\n",
    "    question=question.to(DEVICE)\n",
    ")\n",
    "\n",
    "print(\"Question:             \", my_mega_model.tokenizer.batch_decode(question[\"input_ids\"])[0])\n",
    "print(\"Answer:               \", my_mega_model.tokenizer.batch_decode(answer[\"input_ids\"])[0])\n",
    "print(\"Model decoded answer: \", my_mega_model.decode_from_logits(generated_tokens[\"logits\"]))\n",
    "print(\"Model generated text: \", my_mega_model.tokenizer.batch_decode(generated_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"VQA\",\n",
    "    name=\"Question answering task\",    \n",
    "    config={\n",
    "    \"architecture\": \"Frozen transformers\",\n",
    "    \"dataset\": \"CLEVR_v1.0\",\n",
    "    \"session size\": SESSION_SIZE,\n",
    "    \"batch\": BATCH_SIZE,\n",
    "    \"initial learning rate\": lr\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "predicted_answers = []\n",
    "for iter_num in range(SESSION_SIZE):\n",
    "    imgs, promt, question, answer = train_dataset.get_batch(1)\n",
    "    \n",
    "    my_mega_model.zero_grad()\n",
    "    \n",
    "    output = my_mega_model(\n",
    "        images=imgs.to(DEVICE),\n",
    "        text=promt.to(DEVICE),\n",
    "        model_mode=\"captioning\"\n",
    "    )\n",
    "    \n",
    "    output.loss.backward()\n",
    "    my_mega_model.zero_grad_llm_vit()\n",
    "    opt.step()\n",
    "    \n",
    "    if (iter_num + 1) % 100 == 0:\n",
    "        predicted_answers += [(\n",
    "            my_mega_model.decode_from_logits(output.logits.detach()),\n",
    "            my_mega_model.tokenizer.batch_decode(promt[\"input_ids\"])\n",
    "        )]\n",
    "    \n",
    "    if (iter_num + 1) % (SESSION_SIZE / 10) == 0:\n",
    "        scheduler.step()\n",
    "        \n",
    "    loss_value = output.loss.detach().cpu().item()\n",
    "    writer.add_scalar(\"Loss\", loss_value, iter_num)\n",
    "    wandb.log({\"loss\": loss_value})\n",
    "    \n",
    "wandb.finish()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_answers[-1][0][0])\n",
    "print(predicted_answers[-1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, promt, question, answer = train_dataset.get_batch(BATCH_SIZE)\n",
    "\n",
    "generated_tokens = my_mega_model(\n",
    "    images=imgs.to(DEVICE),\n",
    "    text=question.to(DEVICE),\n",
    "    model_mode=\"captioning\"\n",
    ")\n",
    "\n",
    "generated_output = my_mega_model.generate(\n",
    "    images=imgs.to(DEVICE),\n",
    "    question=question.to(DEVICE)\n",
    ")\n",
    "\n",
    "print(\"Question:             \", my_mega_model.tokenizer.batch_decode(question[\"input_ids\"])[0])\n",
    "print(\"Answer:               \", my_mega_model.tokenizer.batch_decode(answer[\"input_ids\"])[0])\n",
    "print(\"Model decoded answer: \", my_mega_model.decode_from_logits(generated_tokens[\"logits\"]))\n",
    "print(\"Model generated text: \", my_mega_model.tokenizer.batch_decode(generated_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
