{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import typing as tp\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LLM_NAME: str = 'gpt2'\n",
    "VIT_NAME: str = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "VIT_EMB_SIZE = 1000\n",
    "LLM_INP_EMB_SIZE = 768\n",
    "\n",
    "DATA_PATH: str = \"./\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = GPT2LMHeadModel.from_pretrained(LLM_NAME).to(DEVICE)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(LLM_NAME, pad_token=\"<|PAD|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'logit_scale', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vit = CLIPVisionModel.from_pretrained(VIT_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH+\"train\"+\"_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        data_type: str = \"train\",\n",
    "        transform: tp.Optional[albu.Compose | None] = None\n",
    ") -> None:\n",
    "        super().__init__()\n",
    "        self.data_type = data_type\n",
    "        self.data = pd.read_csv(DATA_PATH+data_type+\"_dataframe.csv\")\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.max_question_length = max([\n",
    "            len(ques) for ques in self.data[\"Question\"]\n",
    "        ])\n",
    "        self.max_answer_length = max([\n",
    "            len(ans) for ans in self.data[\"Answer\"]\n",
    "        ])\n",
    "    \n",
    "    def get_batch(self, batch_size: int) -> tp.Tuple:\n",
    "        rand_idxes = np.random.randint(0, len(self.data), batch_size)\n",
    "        \n",
    "        img_paths = self.data.loc[rand_idxes, \"Path\"].to_list()\n",
    "        questions = self.data.loc[rand_idxes, \"Question\"].to_list()\n",
    "        answers = self.data.loc[rand_idxes, \"Answer\"].to_list()\n",
    "        \n",
    "        imgs = torch.cat([\n",
    "            self._get_image(img_path)[None, ...] for img_path in img_paths\n",
    "        ])\n",
    "        \n",
    "        questions_tokens = self.tokenizer(questions, return_tensors=\"pt\", padding=True)\n",
    "        answers_tokens = self.tokenizer(answers, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        return (imgs, questions_tokens, answers_tokens)\n",
    "    \n",
    "    def _get_image(self, image_path: str) -> torch.Tensor:\n",
    "        img = np.array(PIL.Image.open(image_path).convert(\"RGB\"))\n",
    "        if self.transform is None:\n",
    "            raise ValueError(\"Transformation must be at least ToTensor, but None recieved\")\n",
    "        img = self.transform(image=img)[\"image\"].float()\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index) -> tp.Tuple:\n",
    "        img_path, question, answer = data.iloc[index]\n",
    "        \n",
    "        img = self._get_image(img_path)\n",
    "        question_tokens = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        answer_tokens = self.tokenizer(answer, return_tensors=\"pt\")\n",
    "    \n",
    "        return (\n",
    "            img,\n",
    "            question_tokens,\n",
    "            answer_tokens\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[105., 105., 105.,  ..., 102., 102., 102.],\n",
       "           [105., 105., 105.,  ..., 102., 102., 103.],\n",
       "           [105., 105., 105.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [120., 121., 120.,  ..., 150., 149., 148.],\n",
       "           [119., 120., 120.,  ..., 150., 149., 149.],\n",
       "           [120., 120., 120.,  ..., 151., 149., 149.]],\n",
       " \n",
       "          [[105., 105., 105.,  ..., 102., 102., 102.],\n",
       "           [105., 105., 104.,  ..., 102., 102., 103.],\n",
       "           [104., 105., 105.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [119., 120., 119.,  ..., 147., 146., 145.],\n",
       "           [119., 119., 119.,  ..., 147., 146., 147.],\n",
       "           [119., 119., 119.,  ..., 148., 147., 147.]],\n",
       " \n",
       "          [[105., 104., 104.,  ..., 102., 102., 101.],\n",
       "           [104., 105., 104.,  ..., 102., 102., 103.],\n",
       "           [104., 105., 105.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [118., 118., 118.,  ..., 143., 142., 141.],\n",
       "           [117., 118., 118.,  ..., 142., 142., 142.],\n",
       "           [118., 118., 118.,  ..., 142., 142., 142.]]],\n",
       " \n",
       " \n",
       "         [[[106., 105., 106.,  ..., 102., 102., 101.],\n",
       "           [105., 106., 105.,  ..., 102., 102., 103.],\n",
       "           [105., 106., 105.,  ..., 103., 102., 101.],\n",
       "           ...,\n",
       "           [119., 119., 119.,  ..., 153., 152., 152.],\n",
       "           [119., 119., 119.,  ..., 153., 152., 152.],\n",
       "           [118., 119., 119.,  ..., 154., 153., 152.]],\n",
       " \n",
       "          [[106., 105., 106.,  ..., 102., 102., 101.],\n",
       "           [105., 106., 105.,  ..., 102., 102., 103.],\n",
       "           [105., 105., 105.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [118., 118., 118.,  ..., 150., 149., 149.],\n",
       "           [118., 118., 119.,  ..., 151., 149., 149.],\n",
       "           [118., 118., 119.,  ..., 151., 150., 150.]],\n",
       " \n",
       "          [[105., 105., 105.,  ..., 102., 101., 101.],\n",
       "           [105., 105., 105.,  ..., 102., 102., 103.],\n",
       "           [105., 105., 105.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [117., 118., 117.,  ..., 145., 144., 144.],\n",
       "           [117., 117., 118.,  ..., 146., 145., 145.],\n",
       "           [117., 117., 118.,  ..., 146., 145., 145.]]],\n",
       " \n",
       " \n",
       "         [[[106., 106., 106.,  ..., 102., 102., 102.],\n",
       "           [106., 106., 106.,  ..., 102., 102., 103.],\n",
       "           [106., 106., 106.,  ..., 103., 102., 101.],\n",
       "           ...,\n",
       "           [121., 121., 121.,  ..., 150., 148., 148.],\n",
       "           [120., 121., 121.,  ..., 150., 148., 148.],\n",
       "           [120., 121., 121.,  ..., 149., 149., 148.]],\n",
       " \n",
       "          [[106., 105., 106.,  ..., 102., 102., 102.],\n",
       "           [106., 106., 106.,  ..., 102., 102., 103.],\n",
       "           [106., 106., 106.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [120., 120., 120.,  ..., 147., 146., 145.],\n",
       "           [119., 120., 120.,  ..., 147., 146., 146.],\n",
       "           [119., 120., 120.,  ..., 147., 146., 146.]],\n",
       " \n",
       "          [[106., 105., 105.,  ..., 102., 102., 101.],\n",
       "           [105., 106., 105.,  ..., 102., 102., 103.],\n",
       "           [105., 106., 106.,  ..., 103., 101., 101.],\n",
       "           ...,\n",
       "           [118., 118., 118.,  ..., 142., 141., 140.],\n",
       "           [117., 118., 118.,  ..., 142., 141., 141.],\n",
       "           [117., 118., 118.,  ..., 142., 141., 141.]]]]),\n",
       " {'input_ids': tensor([[ 1858,   318,   257,  2266,  1517,   326,   318,   262,   976,  2546,\n",
       "            355,   262, 12768, 36908,  1517,    26,   644,  2587,   318,   340,\n",
       "             30, 50257, 50257],\n",
       "         [ 1858,   318,   257,  2613,   319,   262,  1364,  1735,   286,   262,\n",
       "          36908,  2134,  2157,   262, 36818,  6147,  2512,    26,   703,  1263,\n",
       "            318,   340,    30],\n",
       "         [ 2061,   318,   262,  2587,   286,   262,  1588, 16558,   326,   318,\n",
       "           1364,   286,   262,  1263, 22441,  1517,  2157,   262,  7872,  6147,\n",
       "           2613,    30, 50257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])},\n",
       " {'input_ids': tensor([[25089,   527],\n",
       "         [17470, 50257],\n",
       "         [28469, 50257]]), 'attention_mask': tensor([[1, 1],\n",
       "         [1, 0],\n",
       "         [1, 0]])})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEVRDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_type=\"train\",\n",
    "    transform=transform\n",
    ").get_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = albu.Compose([\n",
    "    albu.Resize(\n",
    "        height=224, \n",
    "        width=224\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_dataset = CLEVRDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_type=\"train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "203 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m imgs, qs, anss \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(imgs\u001b[39m.\u001b[39msize())\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(qs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[55], line 29\u001b[0m, in \u001b[0;36mCLEVRDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTransformation must be at least ToTensor, but None recieved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image\u001b[39m=\u001b[39mimg)[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> 29\u001b[0m question_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(question, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_question_length)\n\u001b[0;32m     30\u001b[0m answer_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(answer, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_answer_length)\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     img,\n\u001b[0;32m     34\u001b[0m     question_tokens,\n\u001b[0;32m     35\u001b[0m     answer_tokens\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2644\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2641\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2645\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2647\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2648\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2649\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2650\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2651\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2652\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2653\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2654\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2655\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2656\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2657\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2658\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2659\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2660\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2661\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2662\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2663\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2708\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2687\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2688\u001b[0m \u001b[39mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2704\u001b[0m \u001b[39m        method).\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 2708\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2709\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2710\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2711\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2712\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2713\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2715\u001b[0m )\n\u001b[0;32m   2717\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2718\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2719\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2736\u001b[0m )\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2380\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2378\u001b[0m     padding_strategy \u001b[39m=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mLONGEST  \u001b[39m# Default to pad to the longest sequence in the batch\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[1;32m-> 2380\u001b[0m     padding_strategy \u001b[39m=\u001b[39m PaddingStrategy(padding)\n\u001b[0;32m   2381\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[0;32m   2382\u001b[0m     padding_strategy \u001b[39m=\u001b[39m padding\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[1;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[39mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# simple value lookup\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39;49m, value)\n\u001b[0;32m    386\u001b[0m \u001b[39m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_create_(\n\u001b[0;32m    388\u001b[0m         value,\n\u001b[0;32m    389\u001b[0m         names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         start\u001b[39m=\u001b[39mstart,\n\u001b[0;32m    394\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\enum.py:718\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    717\u001b[0m             exc\u001b[39m.\u001b[39m__context__ \u001b[39m=\u001b[39m ve_exc\n\u001b[1;32m--> 718\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    719\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m     \u001b[39m# ensure all variables that could hold an exception are destroyed\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     exc \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\enum.py:700\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     exc \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_missing_(value)\n\u001b[0;32m    701\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    702\u001b[0m     exc \u001b[39m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:331\u001b[0m, in \u001b[0;36mExplicitEnum._missing_\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_missing_\u001b[39m(\u001b[39mcls\u001b[39m, value):\n\u001b[1;32m--> 331\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    332\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m is not a valid \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, please select one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_value2member_map_\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: 203 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']"
     ]
    }
   ],
   "source": [
    "for imgs, qs, anss in train_dataloader:\n",
    "    print(imgs.size())\n",
    "    print(qs[\"input_ids\"].size())\n",
    "    print(anss[\"input_ids\"].size())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vit_output = vit(imgs.to(DEVICE))\n",
    "        llm_output = llm(**qs.to(DEVICE), output_hidden_states =True)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367],\n",
       "        [-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367],\n",
       "        [-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367],\n",
       "        ...,\n",
       "        [-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367],\n",
       "        [-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367],\n",
       "        [-0.5499,  0.3904, -1.2417,  ...,  0.0848,  0.1040, -0.3367]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output[\"hidden_states\"][-1][:, 0, -2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1536146756.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MegaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self\n",
    "    ) -> None:\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(LLM_NAME)\n",
    "        self.llm = GPT2LMHeadModel.from_pretrained(LLM_NAME).to(DEVICE)\n",
    "        self.vit = CLIPVisionModel.from_pretrained(VIT_NAME).to(DEVICE)\n",
    "        \n",
    "        self.vit2token = nn.Linear()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        question\n",
    "    ):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = llm.generate(**(q.to(DEVICE)), max_length=40)\n",
    "# tokenizer.decode(generated[0].cpu().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
